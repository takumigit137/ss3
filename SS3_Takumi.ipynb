{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takumigit137/ss3/blob/main/SS3_Takumi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rz5qreVLdrmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n"
      ],
      "metadata": {
        "id": "eWZdJ637eVXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import whisper\n",
        "!pip install pandas\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n"
      ],
      "metadata": {
        "id": "mYck34Ec7hwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKEo4eQIV7LW"
      },
      "outputs": [],
      "source": [
        "# 音声データを4秒ずつに区切る\n",
        "def split_and_trim_wav(input_path, output_path):\n",
        "    # フォルダが存在しない場合は作成\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    # 入力ファイルを読み込む\n",
        "    sound = AudioSegment.from_wav(input_path)\n",
        "\n",
        "    # 分割とトリミング\n",
        "    duration = len(sound)\n",
        "    segment_length = 4000  # 4秒（ミリ秒）\n",
        "    start_time = 0\n",
        "\n",
        "    for i in range(0, duration, segment_length):\n",
        "        end_time = start_time + segment_length\n",
        "        if end_time > duration:\n",
        "            end_time = duration\n",
        "\n",
        "        # 1秒ごとにトリミングして保存\n",
        "        segment = sound[start_time:end_time]\n",
        "        output_file = os.path.join(output_path, f'segment_{i // segment_length + 1}.wav')\n",
        "        segment.export(output_file, format=\"wav\")\n",
        "\n",
        "        # 次のセグメントの開始時刻を更新\n",
        "        start_time = end_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 感情分析を行う関数\n",
        "def analyze_emotion(file_path):\n",
        "    payload = {'apikey': apikey}\n",
        "    data = open(file_path, 'rb')\n",
        "    file = {'wav': data}\n",
        "    res = requests.post(url, params=payload, files=file)\n",
        "    return res.json()"
      ],
      "metadata": {
        "id": "hS6z4kVD8nOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_timelist(segments):\n",
        "  timelist = []\n",
        "  for i in range(len(segments)):\n",
        "    timelist.append(segments[i][\"end\"])\n",
        "  return timelist"
      ],
      "metadata": {
        "id": "pPDmgAuENXl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#盛り上がった４秒間の中間の２秒の時刻をリスト化\n",
        "def joy_times(joy_list):\n",
        "  joy_times_list = []\n",
        "  for joy_index in joy_list:\n",
        "    joy_times_list.append((joy_index+1)*4-2)"
      ],
      "metadata": {
        "id": "B5D9VmFmnuIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_joy_sentence(joy_threshold,timelist,text_result,result_df):\n",
        "  joy_sentence_list = []\n",
        "  #感情が閾値の時のインデックスをリスト化\n",
        "  joy_index_list = result_df[result_df[\"joy\"] > joy_threshold].index\n",
        "  time_index = 0\n",
        "  skip_time_index = 0\n",
        "\n",
        "  for joy_index in joy_index_list:\n",
        "    dict = {}\n",
        "    #盛り上がった４秒間のうち中間の２秒の時刻をリスト化\n",
        "    joy_time = (joy_index+1)*4-2\n",
        "    time_index = skip_time_index\n",
        "    #盛り上がった時間がどこか見つけてその時のsegment_idとjoyの値とtextを\n",
        "    while time_index < len(timelist):\n",
        "      if joy_time <= timelist[time_index]\n",
        "        dict[\"segment_id\"] = time_index\n",
        "        dict[\"joy\"] = result_df[\"joy\"][joy_index]\n",
        "        dict[\"text\"] = text_result[\"segments\"][time_index][\"text\"]\n",
        "        joy_sentence_list.append(dict)\n",
        "        skip_time_index = time_index\n",
        "        break\n",
        "      else:\n",
        "        time_index += 1\n",
        "    #joyの値でソート\n",
        "  return joy_sentence_list = sorted(joy_sentence_list,key=lambda x: x[\"joy\"],reverse=True)"
      ],
      "metadata": {
        "id": "T3LQSA898ACH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力ファイルと出力フォルダのパスを指定\n",
        "input_wav_path = \"/content/drive/MyDrive/SS3/test.wav\"\n",
        "output_folder_path = \"/content/drive/MyDrive/SS3/test_output_segments\"\n",
        "# 音声データを感情分析して，データフレーム型として保存する\n",
        "url = 'https://api.webempath.net/v2/analyzeWav'\n",
        "#apikey = 'API'\n",
        "input_folder = \"/content/drive/MyDrive/SS3/test_output_segments\"\n",
        "output_csv_path = \"/content/drive/MyDrive/SS3/test_emotion_results.csv\"\n"
      ],
      "metadata": {
        "id": "tfEG_6LYAdXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zoomの音声ファイルをwav(16bitのPCM wave形式、サンプリング周波数は11025Hz、チャンネル数はモノラル)にする処理"
      ],
      "metadata": {
        "id": "yTEvtn0pFqcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#文字起こし\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "#結果は辞書型\n",
        "#text_result[\"text\"]に文字列全文が入ってる\n",
        "#text_result[\"segments\"]がリストになってる\n",
        "#リストの各要素がid,text,start(各部分の開始時間),end(各部分の終了時間)の情報持つ辞書型\n",
        "text_result = model.transcribe(input_wav_path)\n",
        "all_text = text_result[\"text\"]\n",
        "timelist = create_timelist(text_result[\"segments\"])"
      ],
      "metadata": {
        "id": "c7qN6ui8GCju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分割とトリミングの実行\n",
        "split_and_trim_wav(input_wav_path, output_folder_path)"
      ],
      "metadata": {
        "id": "vJlsrbTyGHNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データフレームを初期化\n",
        "result_df = pd.DataFrame()\n",
        "\n",
        "# 指定したフォルダ内のすべての.wavファイルに対して感情分析を実行\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        wav_path = os.path.join(input_folder, filename)\n",
        "        result = analyze_emotion(wav_path)\n",
        "\n",
        "        # 結果をデータフレームに追加\n",
        "        df = pd.DataFrame([result])\n",
        "        result_df = result_df.append(df, ignore_index=True)\n",
        "\n",
        "        print(f\"Analysis for {filename} completed. Result added to data frame.\")\n",
        "\n",
        "# データフレームをCSVファイルに保存\n",
        "result_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"CSV file saved at {output_csv_path}\")"
      ],
      "metadata": {
        "id": "Qt0vFgwQ7Gkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#csvファイル読み込んでpandasにする\n",
        "result_df = pd.read_csv(output_csv_path)\n",
        "#joy_sentences:文と閾値以上の感情を対応させた(一応{segment_id,joy,text}の辞書型のリスト)\n",
        "joy_threshold = 30\n",
        "joy_sentences = match_joy_sentence(joy_threshold,timelist,text_result,result_df)"
      ],
      "metadata": {
        "id": "RqzHCSxnGmoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#話題分割\n",
        "!pip install tiktoken\n",
        "!pip install cohere\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "5y1D1GZDfDTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セグメントの区切りに「#」を挿入した文字列を作成\n",
        "text_result_segments= text_result[\"segments\"]\n",
        "segments_text = text_result_segments[0][\"text\"]\n",
        "\n",
        "for i in range(len(text_result_segments)-1):\n",
        "  segments_text= segments_text +\"#\"+ text_result_segments[i+1][\"text\"]\n",
        "segments_text = segments_text + \"#\"\n",
        "segments_text = \"[文章]\" + segments_text"
      ],
      "metadata": {
        "id": "AxiJzFwHXL9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#全文を話題毎に分ける\n",
        "#segmentで分けた文字列をに対して話題分割\n",
        "#responseにapiの応答が格納される\n",
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "  organization='org',api_key='API'\n",
        ")\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4-1106-preview\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"あなたは優秀な会話分析官です。\"},\n",
        "    {\"role\": \"user\", \"content\": \"[文章]は2人の会話です。[文章]は会話の切れ目で「#」が挿入されています。話題が変わるタイミングで「#」の後に「/」を挿入してください。また、話題の数は極力少なくしてください。\"},\n",
        "    {\"role\": \"user\", \"content\":segments_text}\n",
        "]\n",
        ")\n"
      ],
      "metadata": {
        "id": "HHiwAD76HCTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#各話題に含まれるsegment_idのリストを作る関数\n",
        "def make_theme_id_list(string_array):\n",
        "    # 各文字列の中の \"#\" の個数を格納するリスト\n",
        "    counts = []\n",
        "    all_hash_count = 0\n",
        "\n",
        "    # 各文字列に対して処理\n",
        "    for my_string in string_array:\n",
        "        # \"#\" の個数を数えてリストに追加\n",
        "        hash_count = my_string.count(\"#\")\n",
        "        counts.append(list(range(all_hash_count,all_hash_count + hash_count)))\n",
        "        all_hash_count =all_hash_count + hash_count\n",
        "\n",
        "    return counts"
      ],
      "metadata": {
        "id": "7ZyZjXLOXbzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_text=response.choices[0].message.content.split(\"/\") # 要素に各話題を持つ配列\n",
        "theme_ids=make_theme_id_list(split_text) #各話題に含まれるsegment_idのリスト [[0,1],[2,3,4],・・・]"
      ],
      "metadata": {
        "id": "hPbVs_r5XcyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_joy_theme(theme_num,theme_id,joy_sentences):\n",
        "  theme_count = 0\n",
        "  joy_theme_ids = []\n",
        "  while theme_count < theme_num:\n",
        "    for i in range(len(theme_id)):\n",
        "      theme_candidate = theme_id[i]\n",
        "      for j in range(len(thme_id[i])):\n",
        "        if (theme_id[i][j] == joy_sentences[joy_count][\"segment_id\"]) and (theme_candidate not in joy_theme_ids):\n",
        "          joy_theme_ids.append(theme_id[i])\n",
        "          theme_count += 1\n",
        "          break\n",
        "\n",
        "  return joy_theme_ids"
      ],
      "metadata": {
        "id": "Q3oCfkXi1mRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#閾値以上の感情持ってる文がどの話題に属するか特定する\n",
        "#joy_sentences:文と閾値以上の感情を対応させた({segment_id,joy,text}の辞書型のリスト)\n",
        "#持ってくる話題の数\n",
        "theme_num = 1\n",
        "#joy_theme_ids:閾値以上の感情持ってる文が属している話題のid\n",
        "joy_theme_ids = match_joy_theme(theme_num,theme_ids,joy_sentences)"
      ],
      "metadata": {
        "id": "NsC40ULbHIHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#話題を単語にする(単語と感情の大きさがセット)ver2\n",
        "#入力：文章（変数topic_sentenceにstrで入力）, 出力：単語（topic_wordにstrで入力）\n",
        "#response_topicにapiの応答が格納される\n",
        "#感情については単語側での使い道が分からなかったので，とりあえずスルーして単語だけ出すことにしました\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "#topic_sentences:話題の配列(感情の大きい順)[話題A,話題B,話題C...]\n",
        "topic_sentences = []\n",
        "#topic_words:話題を表す単語の配列(感情の大きい順)\n",
        "topic_words = []\n",
        "\n",
        "for theme_id in theme_ids:\n",
        "  topic_sentences.append(split_text[theme_id])\n",
        "  #topic_sentence=\"浅野ともみです。今日の東京株式市場 で、日経平均株価は小幅続伸となっています 。終値は昨日に比べ二十二円七十二銭高の一万一千八十八円五十八銭でした。 \"\n",
        "  topic_sentence = split_text[theme_id]\n",
        "  client = OpenAI(\n",
        "    organization='org',api_key='API'\n",
        "  )\n",
        "  response_topic = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[\n",
        "      # {\"role\": \"system\", \"content\": \"あなたは優秀な会話分析官です。\"},\n",
        "      # {\"role\": \"user\", \"content\": \"[文章]は2人の会話です。この会話の内容を1単語にまとめてください．\"},\n",
        "      {\"role\": \"system\", \"content\": \"あなたは優秀なデートプランナーです．\"},\n",
        "      {\"role\": \"user\", \"content\": \"[文章]は2人の会話です。この会話の内容から，デートスポットを考えるのに使える具体性のある単語を1単語で答えてください\"}\n",
        "      {\"role\": \"user\", \"content\":topic_sentence}\n",
        "  ]\n",
        "  )\n",
        "  print(response_topic.choices[0].message.content)\n",
        "  topic_word = response_topic.choices[0].message.content\n",
        "  #ChatGPTの応答文の長さを見て，10文字以下なら単語のリストに入れる\n",
        "  if len(topic_word) <= 10:\n",
        "    topic_words.append(topic_word)\n",
        "print(topic_words)\n",
        "#株価"
      ],
      "metadata": {
        "id": "57V-v0eqHbkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#単語からスポット提案\n",
        "#入力：topic_words（list型でstr型の単語を格納）（topic_words=[\"中華\",\"イルミネーション\"]）\n",
        "#出力：json_data0,json_data1,...（json型）\n",
        "#単語に地名を追加してハッシュタグ化し，インスタのAPIで検索\n",
        "\n",
        "#１．検索したいハッシュタグのIDを探す\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "for index_spot in range(len(topic_words)):\n",
        "  # 検索したいワード\n",
        "  query = \"東京\"+topic_words[index_spot]\n",
        "\n",
        "  #query = \"中華\"\n",
        "  #query = \"東京\"+query\n",
        "\n",
        "  #IDとアクセストークン\n",
        "  instragramID = \"instragramID\"\n",
        "  ACCESS_TOKEN = \"ACCESS_TOKEN\"\n",
        "\n",
        "\n",
        "  id_search_url = \"https://graph.facebook.com/ig_hashtag_search?user_id=\" + instragramID + \"&q=\" + query +  \"&access_token=\" + ACCESS_TOKEN\n",
        "\n",
        "  response = requests.get(id_search_url)\n",
        "  hash_id = response.json()\n",
        "  print(hash_id)\n",
        "\n",
        "  # ハッシュタグIDを確認\n",
        "  # {'data': [{'id': '17843767135051777'}]}\n",
        "\n",
        "\n",
        "  #２．検索タイプと取得したい項目を設定\n",
        "\n",
        "  # 検索タイプを選択（今回は人気の投稿を探すやつ）\n",
        "  serch_type = \"top_media\"\n",
        "  hash_id2=hash_id[\"data\"][0]\n",
        "  if 'id' in hash_id2:\n",
        "    hash_id3 = hash_id2['id']\n",
        "  else:\n",
        "    hash_id3= 'error'\n",
        "  print(hash_id3)\n",
        "\n",
        "  url = \"https://graph.facebook.com/\" + hash_id3 + \"/\" + serch_type + \"?user_id=\" + instragramID + \"&q=\" + query + \"&access_token=\" + ACCESS_TOKEN + \"&fields=id,media_type,media_url,permalink,like_count,comments_count,caption,timestamp,children{id,media_url}&limit=10\"\n",
        "  #url = \"https://graph.facebook.com/17843767135051777/?user_id=17841449273848018&q=マキアート&access_token=EAAcUTWw6gZBIBO5QxZB6ZAP0wgzpZBlzPiY5ZAkfIh2JAxo00gWv6jAy9zzNjP0Ef631Qaq8704K3HstYrXdUewC53eVuNOyuupYaHxRBq10X5eq6NxlM0Wqd1l2Kn1ZBsRGTBCkcNmGgK6Me4zhAdWtRQPQa2ZCbVlyFGVcgh5PkZBvZB6ytv3GdSQY3eq6OVUkPRQYZBHbQZD&fields=id,media_type,media_url,permalink,like_count,comments_count,caption,timestamp,children{id,media_url}&limit=50\"\n",
        "\n",
        "  #３．データをリクエスト\n",
        "  response = requests.get(url)\n",
        "  #json_data = response.json()\n",
        "  name_json=\"json_data\"+str(index_spot)\n",
        "  globals()[name_json]=response.json()\n",
        "#print(json_data[\"data\"][0])\n"
      ],
      "metadata": {
        "id": "Zr4_1pN8HqWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #4.　jsonの出力をcsvにする\n",
        "# #まだ上のファイル名に対応できてなくて，変数dfに個別にファイル名を入れないといけない状況です．\n",
        "\n",
        "# import pandas as pd\n",
        "# import json\n",
        "# from pandas.io.json import json_normalize\n",
        "\n",
        "# #変換したいJSONファイルを読み込む\n",
        "# df = json_data\n",
        "# print(df)\n",
        "\n",
        "# # read_jsonした結果だとネストしたjsonを展開できないのでnormalizeで展開させる\n",
        "# df_json = json_normalize(df['data'])\n",
        "# df_json.to_csv(\"out.csv\", encoding='utf-8')\n",
        "\n",
        "# print(df_json)\n",
        "\n"
      ],
      "metadata": {
        "id": "2Vn2z6J-S4th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##ChatGPTでデートプランを提案するプログラム\n",
        "#入力：topic_words（list型でstr型の単語を格納）（topic_words=['中華','イルミネーション']）\n",
        "#出力：spot_gpt（list型でstr型の単語を格納）（spot_gpt=['浅草', '六本木']）\n",
        "\n",
        "# from openai import OpenAI\n",
        "\n",
        "\n",
        "# #topic_words:話題を表す単語の配列(感情の大きい順)\n",
        "# spot_gpt = []\n",
        "# topic_words=[\"中華\",\"イルミネーション\"]\n",
        "# for index_spot_gpt in range(len(topic_words)):\n",
        "#   # 検索したいワード\n",
        "#   #topic_sentence=\"浅野ともみです。今日の東京株式市場 で、日経平均株価は小幅続伸となっています 。終値は昨日に比べ二十二円七十二銭高の一万一千八十八円五十八銭でした。 \"\n",
        "#   topic_gpt = topic_words[index_spot_gpt]\n",
        "#   client = OpenAI(\n",
        "#     organization='org',api_key='API'\n",
        "#   )\n",
        "#   response_topic_gpt = client.chat.completions.create(\n",
        "#     model=\"gpt-4-1106-preview\",\n",
        "#   #   messages=[\n",
        "#   #     {\"role\": \"system\", \"content\": \"あなたは優秀な会話分析官です。\"},\n",
        "#   #     {\"role\": \"user\", \"content\": \"[文章]は2人の会話です。この会話の内容を1単語にまとめてください．\"},\n",
        "#   #     {\"role\": \"user\", \"content\":topic_sentence}\n",
        "#   # ]\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"あなたは優秀なデートプランナーです。\"},\n",
        "#         {\"role\": \"user\", \"content\": \"[文章]に関係する東京のデートスポットを1つ、簡潔に提案してください．\"},\n",
        "#         #{\"role\": \"user\", \"content\": \"[文章]に関係する東京のデートスポットを1つ、一単語で提案してください．\"},\n",
        "#         {\"role\": \"user\", \"content\":topic_gpt}\n",
        "#     ]\n",
        "#   )\n",
        "#   print(response_topic_gpt.choices[0].message.content)\n",
        "#   spot_gpt.append(response_topic_gpt.choices[0].message.content)\n",
        "# print(spot_gpt)"
      ],
      "metadata": {
        "id": "SHYj9rLVEopH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}